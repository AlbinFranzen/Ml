{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics for Regression\n",
    "\n",
    "When using metrics to determine error of a training set for regression most often the model overfits the data and produces a lower MSE for the training than the test set. Here we will be looking at methods of compensating for this systematic error.\n",
    "\n",
    "## Mallow's $C_p$\n",
    "\n",
    "Mallows $C_p$ adjusts the RSS of a model with $d$ predictors using the variance of the predictions:\n",
    "\n",
    "$$C_p=\\frac 1 n (\\text{RSS}+2d\\hat{\\sigma}^2)$$\n",
    "\n",
    "Let's implement it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the data\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 + 3 * X + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.8065845639670532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression() #apply model and make predictions\n",
    "lin_reg.fit(X, y)\n",
    "y_pred = lin_reg.predict(X)\n",
    "\n",
    "residuals = y_pred - y\n",
    "RSS = np.sum(residuals**2)\n",
    "mse = 1/len(y)*RSS #MSE formula\n",
    "print(\"MSE: \" + str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8227162552463942"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_C_p = 1/len(y)*(RSS+2*X.shape[1]*np.var(residuals))\n",
    "MSE_C_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akaike information criterion (AIC)\n",
    "\n",
    "AIC works by measuring information loss. If there are $k$ parameters and the maximum value of the log likelihood function is $\\hat{L}$ then the AIC is:\n",
    "\n",
    "$$\\text{AIC}=2k-2\\hat{L}$$\n",
    "\n",
    "A smaller AIC means more information per parameter which can be useful for measuring complexities of models. For least squares regression the log maximum likelihood is $-\\frac{n}{2}\\ln (\\text{RSS}/n)+C$ and $k$ is the number of features plus 1 (2 in our case). For OLS is can even be shown the $C_p$ proportional to AIC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17.49465338856038"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*2+2*len(y)*np.log(RSS/len(y))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian information criterion (BIC)\n",
    "\n",
    "BIC is very similar to AIC but defines information loss as:\n",
    "\n",
    "$$\\text{AIC}=k\\ln n-2\\hat{L}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.780447304204976"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*np.log(len(y))+2*np.log(RSS/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted $R^2$\n",
    "\n",
    "We know that the $R^2$ is defined as 1-RSS/TSS. However to adjust this to select models according to n and d we get:\n",
    "\n",
    "$$\\text{Adjusted }R^2=1-\\frac{\\text{RSS}/(n-d-1)}{\\text{TSS}/(n-1)}$$\n",
    "\n",
    "Unlike the previous accuracy metrics we want to maximise the adjusted $R^2$ instead of minimising it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8805257289083981"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSS = (y-(y-y_pred).mean()).sum()\n",
    "adjusted_R_squared = 1 - (RSS/(len(y)-1-1))/(TSS/(len(y)-1))\n",
    "adjusted_R_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
